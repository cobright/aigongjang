import streamlit as st
import os
import json
import tempfile
import time
import numpy as np
from PIL import Image, ImageDraw, ImageFont # Pillow ëª¨ë“ˆ

# --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì˜ˆì™¸ ì²˜ë¦¬ ---
# 1. Google GenAI (í…ìŠ¤íŠ¸ìš© - êµ¬ë²„ì „ SDK)
import google.generativeai as genai_old

# 2. Google GenAI (ì´ë¯¸ì§€ìš© - ì‹ ë²„ì „ SDK)
# pip install google-genai
try:
    from google import genai
    from google.genai import types
except ImportError:
    st.error("âŒ 'google-genai' ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install google-genai")
    st.stop()

# 3. Google TTS
from google.cloud import texttospeech
from google.oauth2 import service_account

# 4. Pillow íŒ¨ì¹˜ (MoviePy 1.x í˜¸í™˜ì„±)
import PIL.Image
if not hasattr(PIL.Image, 'ANTIALIAS'):
    PIL.Image.ANTIALIAS = PIL.Image.LANCZOS

# 5. MoviePy
from moviepy.editor import *

# 6. .env ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# --- 1. í™˜ê²½ ë° UI ì„¤ì • ---
st.set_page_config(page_title="AI ì˜ìƒ ê³µì¥ (Google Edition)", page_icon="ğŸŒ", layout="wide")
st.title("ğŸŒ AI ì˜ìƒ ê³µì¥ (Gemini 3 Pro Image)")

# --- [ì•ˆì „ì¥ì¹˜] Secrets ì¡°íšŒ í•¨ìˆ˜ ---
def get_secret(key_name):
    """st.secrets -> os.environ ìˆœì„œë¡œ í‚¤ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    try:
        if key_name in st.secrets:
            return st.secrets[key_name]
    except (FileNotFoundError, AttributeError):
        pass
    return os.getenv(key_name)

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì‹œìŠ¤í…œ ì œì–´ê¸°")
    
    gemini_key = get_secret("GOOGLE_API_KEY")
    tts_key_path = get_secret("GOOGLE_APPLICATION_CREDENTIALS") # ë¡œì»¬ íŒŒì¼ ê²½ë¡œ
    tts_key_json = get_secret("GOOGLE_APPLICATION_CREDENTIALS_JSON") # í´ë¼ìš°ë“œìš© JSON ë‚´ìš©
    
    # ìƒíƒœ í‘œì‹œ
    if gemini_key:
        st.success("âœ… Gemini API: Connected")
    else:
        st.error("âŒ Gemini API: Missing Key")
        
    if tts_key_path or tts_key_json:
        st.success("âœ… Google TTS: Connected")
    else:
        st.error("âŒ Google TTS: Missing Credentials")
    
    st.divider()
    
    st.subheader("ğŸ–¼ï¸ ìºë¦­í„° ì„¤ì •")
    default_char = "A young Korean office worker in a suit, simple clean lines, distinct facial features"
    character_desc = st.text_area("ì£¼ì¸ê³µ ì™¸ëª¨ ë¬˜ì‚¬", value=default_char, height=80)
    video_style = st.selectbox("í™”í’ (Style)", ["2D Webtoon Style", "Anime Style", "Realistic Cinematic", "Oil Painting"], index=0)
    
    num_scenes = st.slider("ìƒì„±í•  ì”¬(Scene) ê°œìˆ˜ (ê¶Œì¥: 4ê°œ ì´ìƒ)", 2, 6, 4)

    st.divider()
    st.subheader("ğŸµ ë°°ê²½ìŒì•… (BGM)")
    bgm_mood = st.selectbox(
        "ë¶„ìœ„ê¸° ì„ íƒ", 
        ["ğŸŒ ë°ê³  ê²½ì¾Œí•œ (Bright)", "â˜• ì°¨ë¶„í•œ (Calm)", "ğŸ”¥ ë°•ì§„ê° ë„˜ì¹˜ëŠ” (Epic)", "ğŸ”‡ ì—†ìŒ (Mute)"],
        index=0
    )

# --- 2. í•µì‹¬ ëª¨ë“ˆ í•¨ìˆ˜ ---

def generate_script_json(topic, character_desc, num_scenes):
    """
    [Text] Gemini (Old SDK): 
    ê¸°ì¡´ì˜ ë‹¨ìˆœ ìƒì„± ë°©ì‹ì„ ë²„ë¦¬ê³ , 'Hook-Body-Conclusion' êµ¬ì¡°ë¥¼ ê°•ì œí•©ë‹ˆë‹¤.
    """
    if not gemini_key: return None
    
    try:
        genai_old.configure(api_key=gemini_key)
        
        # ìš”ì²­í•˜ì‹  ëŒ€ë¡œ 2.5 ë²„ì „ ì‚¬ìš© (ì£¼ì˜: 2.5ê°€ ì•„ì§ ë°°í¬ ì „ì´ë¼ë©´ 1.5ë‚˜ 2.0-flash-exp ì‚¬ìš©)
        model = genai_old.GenerativeModel('gemini-2.5-flash') 
        
        # êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ì„¤ê³„
        prompt = f"""
        You are an expert Content Creator for viral YouTube Shorts and TikTok.
        Your goal is to create a script for the topic: '{topic}'
        
        [STRUCTURE STRATEGY - MUST FOLLOW]
        Organize the {num_scenes} scenes strictly according to this flow:
        1. **HOOK**: Start with a question or shock.
        2. **BODY**: Explain the core story.
        3. **OUTRO**: Conclusion and CTA.

        [STRICT LANGUAGE RULES]
        1. "narrative": Must be in **KOREAN (í•œêµ­ì–´)**. Conversational style.
        2. "visual_prompt": Must be in **KOREAN (í•œêµ­ì–´)**.
        
        [DYNAMIC VISUAL INSTRUCTION - IMPORTANT]
        To make the video dynamic, **you MUST provide 2 or 3 different visual descriptions per scene**, separated by " || ".
        - Example: "Close up of a man eating kimchi || Wide shot of the restaurant || The man giving a thumbs up"
        - This will generate 3 images shown in sequence for this one scene.
        
        [OUTPUT FORMAT]
        Return ONLY valid JSON:
        {{
          "video_title": "Title in Korean",
          "scenes": [
            {{ 
               "seq": 1, 
               "section": "HOOK",
               "narrative": "Korean voiceover...", 
               "visual_prompt": "Image Desc 1 || Image Desc 2" 
            }},
            ...
          ]
        }}
        """
        
        response = model.generate_content(prompt)
        text = response.text.strip()
        
        # ë§ˆí¬ë‹¤ìš´ ì œê±° (ì•ˆì „ì¥ì¹˜)
        if text.startswith("```json"):
            text = text[7:]
        if text.endswith("```"):
            text = text[:-3]
            
        return json.loads(text)
        
    except Exception as e:
        st.error(f"ğŸ§  Gemini ê¸°íš ì˜¤ë¥˜: {e}")
        return None

def generate_image_google(prompt, filename):
    """[Image] Gemini (New SDK): ì´ë¯¸ì§€ ìƒì„±"""
    if not gemini_key: return None
    
    output_path = os.path.join(tempfile.gettempdir(), filename)

    try:
        client = genai.Client(api_key=gemini_key)
        # ëª¨ë¸ ID í™•ì¸ í•„ìš” (imagen-3.0-generate-001 ë˜ëŠ” gemini-pro-vision ë“± ìƒí™©ì— ë§ê²Œ)
        # í˜„ì¬ ì½”ë“œì˜ gemini-3-pro-image-previewëŠ” Preview ê¶Œí•œì´ ìˆì–´ì•¼ ì‘ë™í•©ë‹ˆë‹¤.
        model_id = "gemini-3-pro-image-preview" # í˜¹ì€ "imagen-3.0-generate-001"
        
        # *ì°¸ê³ : Gemini 2.0 Flash ExpëŠ” ì´ë¯¸ì§€ ìƒì„±ì„ ì§€ì›í•˜ì§€ë§Œ, 
        # ì „ìš© Imagen ëª¨ë¸ì„ ì“´ë‹¤ë©´ ì½”ë“œê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
        # ì—¬ê¸°ì„œëŠ” ì‘ì„±í•´ì£¼ì‹  ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
        
        contents = [types.Content(role="user", parts=[types.Part.from_text(text=prompt)])]
        generate_content_config = types.GenerateContentConfig(
            response_modalities=["IMAGE"],
            image_config=types.ImageConfig(image_size="1K"), # í˜¹ì€ "1024x1024" ë“± SDK ë²„ì „ì— ë”°ë¼ ë‹¤ë¦„
        )

        response = client.models.generate_content(
            model=model_id,
            contents=contents,
            config=generate_content_config,
        )
        
        if response.candidates and response.candidates[0].content.parts:
            for part in response.candidates[0].content.parts:
                if part.inline_data and part.inline_data.data:
                    with open(output_path, "wb") as f:
                        f.write(part.inline_data.data)
                    return output_path
        
        st.warning("âš ï¸ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (ëª¨ë¸ ì‘ë‹µ ì—†ìŒ).")
        return None

    except Exception as e:
        st.error(f"ğŸ¨ Google ì´ë¯¸ì§€ ìƒì„± ì˜¤ë¥˜: {e}")
        return None

def generate_audio(text, filename):
    """[Voice] Google TTS"""
    output_path = os.path.join(tempfile.gettempdir(), filename)
    
    # ì¸ì¦ ì²˜ë¦¬ ë¡œì§ ë‹¨ìˆœí™” ë° ê°•í™”
    credentials = None
    
    try:
        # 1. Secret/Envì— JSON ë‚´ìš©ì´ í†µì§¸ë¡œ ìˆëŠ” ê²½ìš° (Streamlit Cloud ê¶Œì¥)
        if tts_key_json:
            try:
                creds_info = json.loads(tts_key_json, strict=False)
                credentials = service_account.Credentials.from_service_account_info(creds_info)
            except json.JSONDecodeError:
                st.error("TTS JSON í‚¤ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return None
        
        # 2. ë¡œì»¬ íŒŒì¼ ê²½ë¡œê°€ ìˆëŠ” ê²½ìš°
        elif tts_key_path and os.path.exists(tts_key_path):
            credentials = service_account.Credentials.from_service_account_file(tts_key_path)
            
        else:
            # í•˜ë“œì½”ë”©ëœ íŒŒì¼ëª…ì€ ì œê±°í•˜ê³  ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
            st.error("âŒ TTS ì¸ì¦ í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (.env ë˜ëŠ” secrets.toml í™•ì¸)")
            return None

        client = texttospeech.TextToSpeechClient(credentials=credentials)
        input_text = texttospeech.SynthesisInput(text=text)
        voice = texttospeech.VoiceSelectionParams(language_code="ko-KR", name="ko-KR-Standard-C", ssml_gender=texttospeech.SsmlVoiceGender.MALE)
        audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)
        
        response = client.synthesize_speech(input=input_text, voice=voice, audio_config=audio_config)
        with open(output_path, "wb") as out:
            out.write(response.audio_content)
        return output_path
        
    except Exception as e:
        st.error(f"ğŸ™ï¸ TTS ì˜¤ë¥˜: {e}")
        return None

def create_zoom_effect(clip, zoom_ratio=0.04):
    """Zoom Effect (OpenCV í•„ìš”)"""
    try:
        import cv2
    except ImportError:
        # OpenCV ì—†ìœ¼ë©´ íš¨ê³¼ ì—†ì´ ì›ë³¸ ë¦¬í„´
        return clip

    def effect(get_frame, t):
        img = get_frame(t)
        h, w = img.shape[:2]
        scale = 1 + zoom_ratio * t
        new_w, new_h = int(w * scale), int(h * scale)
        img_resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
        x = (new_w - w) // 2
        y = (new_h - h) // 2
        return img_resized[y:y+h, x:x+w]

    return clip.fl(effect)

def get_bgm_path(mood):
    """
    ì„ íƒëœ ë¶„ìœ„ê¸°(mood)ì— ë§ëŠ” ë¬´ë£Œ BGMì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    (ì €ì‘ê¶Œ ë¬´ë£Œ ìŒì›: Kevin MacLeod ë“± Open Source í™œìš©)
    """
    if mood == "ğŸ”‡ ì—†ìŒ (Mute)":
        return None
        
    # ë¶„ìœ„ê¸°ë³„ ìŒì› URL (ì•ˆì •ì ì¸ GitHub Raw ì†ŒìŠ¤ ë“±ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìœ¼ë‚˜, ì˜ˆì‹œë¡œ ë¬´ë£Œ ìŒì› ë§í¬ ì‚¬ìš©)
    # ì‹¤ì œ ìš´ì˜ ì‹œì—ëŠ” ë³¸ì¸ì˜ ì„œë²„ë‚˜ S3 ë§í¬ë¡œ êµì²´í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    bgm_urls = {
        "ğŸŒ ë°ê³  ê²½ì¾Œí•œ (Bright)": "https://www.bensound.com/bensound-music/bensound-ukulele.mp3", # ì˜ˆì‹œ URL
        "â˜• ì°¨ë¶„í•œ (Calm)": "https://www.bensound.com/bensound-music/bensound-slowmotion.mp3", 
        "ğŸ”¥ ë°•ì§„ê° ë„˜ì¹˜ëŠ” (Epic)": "https://www.bensound.com/bensound-music/bensound-evolution.mp3"
    }
    
    url = bgm_urls.get(mood)
    if not url: return None
    
    # ì„ì‹œ íŒŒì¼ëª… (ë¶„ìœ„ê¸°ë³„ë¡œ ìºì‹±)
    filename = f"bgm_{mood[:2]}.mp3"
    filepath = os.path.join(tempfile.gettempdir(), filename)
    
    # ì´ë¯¸ ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼ì´ ìˆìœ¼ë©´ ì¬ì‚¬ìš© (ì†ë„ í–¥ìƒ)
    if os.path.exists(filepath):
        return filepath
        
    try:
        # User-Agent í—¤ë” ì¶”ê°€ (ì°¨ë‹¨ ë°©ì§€)
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        with open(filepath, "wb") as f:
            f.write(response.content)
        return filepath
    except Exception as e:
        st.warning(f"BGM ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def get_korean_font():
    """
    í•œê¸€ í°íŠ¸(ë‚˜ëˆ”ê³ ë”•)ë¥¼ ì„ì‹œ í´ë”ì— ë‹¤ìš´ë¡œë“œí•˜ì—¬ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    (í•œê¸€ ê¹¨ì§ ë°©ì§€ìš©)
    """
    font_url = "https://github.com/google/fonts/raw/main/ofl/nanumgothic/NanumGothic-Bold.ttf"
    font_path = os.path.join(tempfile.gettempdir(), "NanumGothic-Bold.ttf")
    
    if not os.path.exists(font_path):
        try:
            response = requests.get(font_url)
            with open(font_path, "wb") as f:
                f.write(response.content)
        except Exception:
            return None # ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í°íŠ¸ ì‹œë„
            
    return font_path

def create_subtitle(text, duration, font_path):
    """
    [ë¬´ì„¤ì¹˜ ë²„ì „] Pillowë¥¼ ì‚¬ìš©í•˜ì—¬ ìë§‰ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ImageMagickì´ í•„ìš” ì—†ì–´ ì˜¤ë¥˜ê°€ ë‚˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    try:
        # 1. ìº”ë²„ìŠ¤ í¬ê¸° ì„¤ì • (HD í•´ìƒë„ ê¸°ì¤€)
        w, h = 1280, 720
        # íˆ¬ëª… ë°°ê²½ ì´ë¯¸ì§€ ìƒì„± (RGBA)
        img = Image.new('RGBA', (w, h), (255, 255, 255, 0))
        draw = ImageDraw.Draw(img)
        
        # 2. í°íŠ¸ ë¡œë“œ
        font_size = 50
        try:
            if font_path:
                font = ImageFont.truetype(font_path, font_size)
            else:
                # í°íŠ¸ ê²½ë¡œ ì—†ìœ¼ë©´ ê¸°ë³¸ í°íŠ¸ (í•œê¸€ ê¹¨ì§ˆ ìˆ˜ ìˆìŒ)
                font = ImageFont.load_default()
        except Exception:
            font = ImageFont.load_default()

        # 3. í…ìŠ¤íŠ¸ ì¤„ë°”ê¿ˆ ì²˜ë¦¬ (ê°„ë‹¨ ë²„ì „)
        if len(text) > 25:
            text = text[:25] + "\n" + text[25:]
        
        # 4. í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚° ë° ìœ„ì¹˜ ì¡ê¸° (ì¤‘ì•™ í•˜ë‹¨)
        # textbboxëŠ” Pillow ìµœì‹  ë²„ì „ ê¸°ì¤€
        left, top, right, bottom = draw.textbbox((0, 0), text, font=font)
        text_w = right - left
        text_h = bottom - top
        
        x = (w - text_w) / 2
        y = h - 100 # ë°‘ì—ì„œ 100px ìœ„
        
        # 5. ê¸€ì í…Œë‘ë¦¬(Stroke) ê·¸ë¦¬ê¸° (ê²€ì€ìƒ‰)
        stroke_width = 3
        draw.text((x, y), text, font=font, fill="white", stroke_width=stroke_width, stroke_fill="black")

        # 6. MoviePy í´ë¦½ìœ¼ë¡œ ë³€í™˜
        # Pillow ì´ë¯¸ì§€ -> Numpy ë°°ì—´ -> ImageClip
        return ImageClip(np.array(img)).set_duration(duration)

    except Exception as e:
        st.error(f"ìë§‰ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


# --- 3. ë©”ì¸ ì‹¤í–‰ ì»¨íŠ¸ë¡¤ëŸ¬ ---

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ìƒˆë¡œê³ ì¹¨ í•´ë„ ë°ì´í„° ìœ ì§€)
if "script_data" not in st.session_state:
    st.session_state["script_data"] = None
if "step" not in st.session_state:
    st.session_state["step"] = 1

st.divider()
st.header("Step 1. ê¸°íšì•ˆ ì‘ì„±")
topic = st.text_input("ì˜ìƒ ì£¼ì œ (Topic)", placeholder="ì˜ˆ: ì§‘ì—ì„œ ë§Œë“œëŠ” ìŠ¤íƒ€ë²…ìŠ¤ ëŒì²´ë¼ë–¼ ë ˆì‹œí”¼")

# [ë²„íŠ¼ 1] ê¸°íšì•ˆ ìƒì„±
if st.button("ğŸ’¡ 1. ê¸°íšì•ˆ(ëŒ€ë³¸) ìƒì„±í•˜ê¸°", type="primary", use_container_width=True):
    if not topic:
        st.warning("ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()
        
    with st.spinner("ğŸ§  Geminiê°€ ê¸°ìŠ¹ì „ê²°(Hook-Body-CTA) êµ¬ì¡°ë¡œ ê¸°íš ì¤‘ì…ë‹ˆë‹¤..."):
        # 1ë‹¨ê³„ì—ì„œ ë§Œë“  êµ¬ì¡°í™”ëœ í•¨ìˆ˜ í˜¸ì¶œ
        script_data = generate_script_json(topic, character_desc, num_scenes)
        
        if script_data:
            st.session_state["script_data"] = script_data
            st.session_state["step"] = 2
            st.rerun() # í™”ë©´ ê°±ì‹ 
        else:
            st.error("ê¸°íšì•ˆ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

# [UI] ëŒ€ë³¸ ìˆ˜ì • ë° í™•ì • ë‹¨ê³„
if st.session_state["step"] >= 2 and st.session_state["script_data"]:
    st.divider()
    st.header("Step 2. ëŒ€ë³¸ ë° ì—°ì¶œ ìˆ˜ì •")
    st.info("ğŸ’¡ ê¿€íŒ: 'ê·¸ë¦¼ ë¬˜ì‚¬' ì¹¸ì— ` || ` ê¸°í˜¸ë¥¼ ë„£ìœ¼ë©´ ì»·ì´ ìª¼ê°œì§‘ë‹ˆë‹¤. (ì˜ˆ: ë‚¨ìì˜ ì–¼êµ´ || ë†€ë¼ëŠ” í‘œì •)")

    data = st.session_state["script_data"]
    scenes = data.get("scenes", [])
    
    # í¼(Form)ì„ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆì— ë°ì´í„° ìˆ˜ì§‘
    with st.form("script_edit_form"):
        # íƒ€ì´í‹€ ìˆ˜ì •
        new_title = st.text_input("ì˜ìƒ ì œëª©", value=data.get("video_title", ""))
        
        edited_scenes = []
        for i, scene in enumerate(scenes):
            st.subheader(f"ğŸ¬ Scene {scene['seq']} ({scene.get('section', 'General')})")
            
            # ë ˆì´ì•„ì›ƒ 2ë‹¨ ë¶„ë¦¬ (ì™¼ìª½: ëŒ€ë³¸ / ì˜¤ë¥¸ìª½: ê·¸ë¦¼ ë¬˜ì‚¬)
            col1, col2 = st.columns(2)
            
            with col1:
                # keyë¥¼ ê³ ìœ í•˜ê²Œ ì§€ì •í•´ì•¼ í•¨
                new_narrative = st.text_area(
                    label="ğŸ—£ï¸ ë‚´ë ˆì´ì…˜ (í•œêµ­ì–´)", 
                    value=scene['narrative'], 
                    height=100,
                    key=f"narr_area_{i}"
                )
            
            with col2:
                new_visual = st.text_area(
                    label="ğŸ–¼ï¸ ê·¸ë¦¼ ë¬˜ì‚¬ (ì˜ì–´ ê¶Œì¥)", 
                    value=scene['visual_prompt'], 
                    height=100,
                    key=f"vis_area_{i}"
                )
            
            # ìˆ˜ì •ëœ ë‚´ìš©ì„ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì„ ì¤€ë¹„ (ì‹¤ì œ ì—…ë°ì´íŠ¸ëŠ” ë²„íŠ¼ í´ë¦­ ì‹œ ì²˜ë¦¬)
            # ì—¬ê¸°ì„œëŠ” í¼ ë‚´ë¶€ë¼ UIë§Œ ë³´ì—¬ì£¼ê³ , ë°ì´í„° ì²˜ë¦¬ëŠ” ì•„ë˜ submit ë²„íŠ¼ ì´í›„ì— í•¨.
            
        # [ë²„íŠ¼ 2] ì˜ìƒ ìƒì„± ì‹œì‘
        generate_btn = st.form_submit_button("ğŸ¬ 2. ì´ ë‚´ìš©ìœ¼ë¡œ ì˜ìƒ ë§Œë“¤ê¸° (Start Generation)", type="primary", use_container_width=True)

    # í¼ ì œì¶œ ë²„íŠ¼ì´ ëˆŒë ¸ì„ ë•Œ ì‹¤í–‰
    if generate_btn:
        # ìˆ˜ì •ëœ ë°ì´í„° ìˆ˜ì§‘
        final_scenes = []
        for i, org_scene in enumerate(scenes):
            final_scenes.append({
                "seq": org_scene['seq'],
                "narrative": st.session_state[f"narr_area_{i}"],      # ìœ„ text_areaì˜ keyê°’ìœ¼ë¡œ ì½ì–´ì˜´
                "visual_prompt": st.session_state[f"vis_area_{i}"]   # ìœ„ text_areaì˜ keyê°’ìœ¼ë¡œ ì½ì–´ì˜´
            })
            
        # ë³¸ê²©ì ì¸ ìƒì„± ì‹œì‘
        status_box = st.status("ğŸ—ï¸ ì˜ìƒ ì œì‘ ê³µì¥ ê°€ë™ ì¤‘...", expanded=True)
        
        # --- Phase 2: ë‹¤ì´ë‚˜ë¯¹ ì»· ìƒì„± (ì»· ìª¼ê°œê¸° ì ìš©) ---
        status_box.write("ğŸ¨ Phase 2: ë‹¤ì´ë‚˜ë¯¹ ì»·(ì´ë¯¸ì§€ ë¶„í• ) ë° ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...")
        progress_bar = st.progress(0)
        generated_clips = []
        
        for i, scene in enumerate(final_scenes):
            idx = scene['seq']
            status_box.write(f"  - Scene {idx} ì‘ì—… ì¤‘...")
            
            timestamp = int(time.time())
            
            # 1. ì˜¤ë””ì˜¤ ë¨¼ì € ìƒì„± (ê¸¸ì´ë¥¼ ì•Œì•„ì•¼ ì»·ì„ ë‚˜ëˆŒ ìˆ˜ ìˆìŒ)
            aud_name = f"aud_{idx}_{timestamp}.mp3"
            aud_path = generate_audio(scene['narrative'], aud_name)
            
            if not aud_path:
                continue # ì˜¤ë””ì˜¤ ì‹¤íŒ¨ ì‹œ ê±´ë„ˆëœ€
                
            audio_clip = AudioFileClip(aud_path)
            scene_duration = audio_clip.duration
            
            # 2. ë¹„ì£¼ì–¼ í”„ë¡¬í”„íŠ¸ ë¶„ì„ ('||' ê¸°ì¤€ìœ¼ë¡œ ìª¼ê°œê¸°)
            # ì˜ˆ: "ê³ ì–‘ì´ê°€ ë°¥ë¨¹ìŒ || ê³ ì–‘ì´ê°€ ì " -> ["ê³ ì–‘ì´ê°€ ë°¥ë¨¹ìŒ", "ê³ ì–‘ì´ê°€ ì "]
            raw_prompts = scene['visual_prompt'].split('||')
            visual_prompts = [p.strip() for p in raw_prompts if p.strip()]
            
            # ë§Œì•½ ìª¼ê°¤ ê²Œ ì—†ìœ¼ë©´ ê·¸ëƒ¥ 1ê°œë¡œ ì²˜ë¦¬
            if not visual_prompts:
                visual_prompts = [scene['visual_prompt']]
            
            # ì»· ë‹¹ ì§€ì† ì‹œê°„ ê³„ì‚° (ì´ ì‹œê°„ / ì´ë¯¸ì§€ ê°œìˆ˜)
            # ì˜ˆ: ì˜¤ë””ì˜¤ 6ì´ˆ, ì´ë¯¸ì§€ 3ì¥ì´ë©´ -> ê° 2ì´ˆì”© ë³´ì—¬ì¤Œ
            clip_duration = scene_duration / len(visual_prompts)
            
            scene_sub_clips = [] # ì´ ì”¬ì„ êµ¬ì„±í•  ì‘ì€ ì¡°ê° ì˜ìƒë“¤
            
            # 3. ê° ì»· ë³„ë¡œ ì´ë¯¸ì§€ ìƒì„± ë° í´ë¦½ ë§Œë“¤ê¸°
            for sub_idx, prompt in enumerate(visual_prompts):
                img_name = f"img_{idx}_{sub_idx}_{timestamp}.png"
                status_box.write(f"    â”” ì»· {sub_idx+1}/{len(visual_prompts)}: {prompt[:20]}...")
                
                img_path = generate_image_google(prompt, img_name)
                
                if img_path:
                    try:
                        # ì´ë¯¸ì§€ í´ë¦½ ìƒì„± (ê³„ì‚°ëœ ì‹œê°„ë§Œí¼)
                        sub_clip = ImageClip(img_path).set_duration(clip_duration).resize(height=720)
                        
                        # ì¤Œ íš¨ê³¼ë„ ê°ê° ì ìš© (ë” ì—­ë™ì ì„)
                        sub_clip = create_zoom_effect(sub_clip)
                        scene_sub_clips.append(sub_clip)
                    except Exception as e:
                        st.warning(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 4. ì¡°ê° ì˜ìƒë“¤ í•©ì¹˜ê¸° + ì˜¤ë””ì˜¤ ì…íˆê¸°
            if scene_sub_clips:
                try:
                    # ì´ë¯¸ì§€ 1, 2, 3ì„ ìˆœì„œëŒ€ë¡œ ì´ì–´ ë¶™ì„
                    full_scene_clip = concatenate_videoclips(scene_sub_clips, method="compose")
                    
                    # ì˜¤ë””ì˜¤ ì„¤ì • (ê¸¸ì´ê°€ ë¯¸ì„¸í•˜ê²Œ ì•ˆ ë§ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë§ì¶¤)
                    full_scene_clip = full_scene_clip.set_audio(audio_clip)
                    
                    generated_clips.append(full_scene_clip)
                except Exception as e:
                    st.error(f"Scene {idx} í•©ì¹˜ê¸° ì‹¤íŒ¨: {e}")
            
            progress_bar.progress((i + 1) / len(final_scenes))

        # Phase 3: Final Rendering (BGM Mixing ì¶”ê°€)
        if generated_clips:
            status_box.write("ğŸ¬ Phase 3: ì˜ìƒ í•©ì¹˜ê¸° ë° BGM ë¯¹ì‹± ì¤‘...")
            try:
                # 1. ì»· í¸ì§‘ëœ ì˜ìƒ ì—°ê²°
                final_video = concatenate_videoclips(generated_clips, method="compose")
                
                # 2. BGM ì²˜ë¦¬ ë¡œì§
                bgm_path = get_bgm_path(bgm_mood)
                
                if bgm_path:
                    try:
                        # BGM ë¡œë“œ
                        bgm_clip = AudioFileClip(bgm_path)
                        
                        # ì˜ìƒ ê¸¸ì´ì— ë§ì¶° BGM ë°˜ë³µ(Loop) ë˜ëŠ” ìë¥´ê¸°
                        # (ì˜ìƒë³´ë‹¤ BGMì´ ì§§ìœ¼ë©´ ë°˜ë³µ, ê¸¸ë©´ ìë¦„)
                        if bgm_clip.duration < final_video.duration:
                            # ì§ìˆ˜ ë²ˆ ë°˜ë³µí•´ì„œ ì¶©ë¶„íˆ ê¸¸ê²Œ ë§Œë“¦
                            loop_count = int(final_video.duration // bgm_clip.duration) + 2
                            bgm_clip = concatenate_audioclips([bgm_clip] * loop_count)
                        
                        bgm_clip = bgm_clip.set_duration(final_video.duration)
                        
                        # ë³¼ë¥¨ ì¡°ì ˆ (ê°€ì¥ ì¤‘ìš”!)
                        # ëª©ì†Œë¦¬(Voice)ëŠ” 100%, BGMì€ 10%~15% ìˆ˜ì¤€ìœ¼ë¡œ ë‚®ì¶¤
                        voice_clip = final_video.audio.volumex(1.0) # ì›ë³¸ ëª©ì†Œë¦¬ ìœ ì§€
                        bgm_clip = bgm_clip.volumex(0.15)           # ë°°ê²½ìŒì•… 15% (ì€ì€í•˜ê²Œ)
                        
                        # í˜ì´ë“œ ì•„ì›ƒ (ëë‚  ë•Œ 2ì´ˆê°„ ì„œì„œíˆ ì‘ì•„ì§)
                        bgm_clip = bgm_clip.audio_fadeout(2)
                        
                        # ì˜¤ë””ì˜¤ í•©ì¹˜ê¸° (ëª©ì†Œë¦¬ + BGM)
                        final_audio = CompositeAudioClip([voice_clip, bgm_clip])
                        final_video = final_video.set_audio(final_audio)
                        
                    except Exception as e:
                        st.warning(f"BGM í•©ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ(ì˜ìƒì€ BGM ì—†ì´ ìƒì„±ë©ë‹ˆë‹¤): {e}")

                # 3. ìµœì¢… ë‚´ë³´ë‚´ê¸°
                safe_title = "".join([c for c in new_title if c.isalnum()]).strip() or "output"
                output_path = os.path.join(tempfile.gettempdir(), f"{safe_title}_final.mp4")
                
                final_video.write_videofile(output_path, fps=24, codec='libx264', audio_codec='aac', preset='ultrafast')
                
                status_box.update(label="âœ… ì˜ìƒ ì™„ì„±!", state="complete", expanded=False)
                st.balloons()
                st.success(f"ğŸ‰ '{new_title}' ì˜ìƒì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤! (BGM: {bgm_mood})")
                st.video(output_path)
                
            except Exception as e:
                st.error(f"ë Œë”ë§ ì˜¤ë¥˜: {e}")